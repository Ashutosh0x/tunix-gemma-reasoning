{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#  GRPO Training - On-Policy RL\n",
                "\n",
                "Run Group Relative Policy Optimization to improve reasoning quality.\n",
                "\n",
                "**Time estimate:** ~4-5 hours on Kaggle TPU\n",
                "\n",
                "**Prerequisites:** Completed SFT training (notebook 02)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import time\n",
                "import re\n",
                "from datetime import datetime\n",
                "\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "import numpy as np\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "print(f\"JAX devices: {jax.device_count()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. GRPO Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CFG = {\n",
                "    # Model\n",
                "    'model_name': 'google/gemma-3-1b-it',\n",
                "    'sft_checkpoint': 'checkpoints/sft/sft_epoch_2',\n",
                "    \n",
                "    # GRPO\n",
                "    'num_generations': 4,  # G: responses per prompt\n",
                "    'temperature': 0.8,\n",
                "    'learning_rate': 3e-6,\n",
                "    'kl_coef': 0.03,\n",
                "    'clip_epsilon': 0.2,\n",
                "    \n",
                "    # Training\n",
                "    'num_updates': 2000,\n",
                "    'batch_size': 2,  # prompts per batch\n",
                "    'max_length': 1024,\n",
                "    \n",
                "    # Reward weights\n",
                "    'w_correct': 0.60,\n",
                "    'w_trace': 0.25,\n",
                "    'w_conf': 0.15,\n",
                "    \n",
                "    # Logging\n",
                "    'log_steps': 20,\n",
                "    'save_minutes': 20,\n",
                "    'eval_steps': 100,\n",
                "    'seed': 42\n",
                "}\n",
                "\n",
                "print(\" GRPO Configuration:\")\n",
                "for k, v in CFG.items():\n",
                "    print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model & Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer = AutoTokenizer.from_pretrained(CFG['model_name'])\n",
                "\n",
                "# Load SFT checkpoint\n",
                "# model = load_checkpoint(CFG['sft_checkpoint'])\n",
                "print(f\" Loaded SFT checkpoint: {CFG['sft_checkpoint']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load training prompts with references\n",
                "def load_jsonl(path):\n",
                "    data = []\n",
                "    with open(path, 'r', encoding='utf-8') as f:\n",
                "        for line in f:\n",
                "            data.append(json.loads(line.strip()))\n",
                "    return data\n",
                "\n",
                "train_data = load_jsonl('data/prepared/train.jsonl')\n",
                "val_data = load_jsonl('data/prepared/valid.jsonl')\n",
                "\n",
                "print(f\" Loaded {len(train_data)} train, {len(val_data)} val prompts\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Reward Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reward functions (from src/rewards.py)\n",
                "\n",
                "TRANSITION_WORDS = [\n",
                "    'therefore', 'thus', 'hence', 'so', 'because', 'first', 'second',\n",
                "    'step', 'next', 'then', 'finally', 'since', 'given'\n",
                "]\n",
                "\n",
                "def extract_answer(text):\n",
                "    match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL | re.IGNORECASE)\n",
                "    return match.group(1).strip() if match else None\n",
                "\n",
                "def extract_reasoning(text):\n",
                "    match = re.search(r'<reasoning>(.*?)</reasoning>', text, re.DOTALL | re.IGNORECASE)\n",
                "    return match.group(1).strip() if match else None\n",
                "\n",
                "def correctness_score(pred_text, ref_answer):\n",
                "    pred_ans = extract_answer(pred_text)\n",
                "    if pred_ans is None:\n",
                "        return 0.0\n",
                "    pred_norm = pred_ans.lower().strip()\n",
                "    ref_norm = ref_answer.lower().strip()\n",
                "    try:\n",
                "        pred_num = float(re.sub(r'[^\\d.\\-]', '', pred_norm))\n",
                "        ref_num = float(re.sub(r'[^\\d.\\-]', '', ref_norm))\n",
                "        return 1.0 if abs(pred_num - ref_num) < 0.01 else 0.0\n",
                "    except:\n",
                "        return 1.0 if pred_norm == ref_norm else 0.0\n",
                "\n",
                "def trace_structure_score(text):\n",
                "    score = 0.0\n",
                "    if '<reasoning>' in text.lower() and '</reasoning>' in text.lower():\n",
                "        score += 0.2\n",
                "    if '<answer>' in text.lower() and '</answer>' in text.lower():\n",
                "        score += 0.2\n",
                "    \n",
                "    reasoning = extract_reasoning(text)\n",
                "    if reasoning:\n",
                "        steps = [s for s in re.split(r'[.\\n]', reasoning) if len(s.strip()) > 10]\n",
                "        score += min(0.4, len(steps) / 3.0 * 0.4)\n",
                "        trans_count = sum(1 for w in TRANSITION_WORDS if w in reasoning.lower())\n",
                "        score += min(0.2, trans_count / 3.0 * 0.2)\n",
                "    \n",
                "    return min(1.0, score)\n",
                "\n",
                "def composite_reward(pred_text, ref_answer):\n",
                "    correct = correctness_score(pred_text, ref_answer)\n",
                "    trace = trace_structure_score(pred_text)\n",
                "    conf = 0.5  # Placeholder for confidence\n",
                "    \n",
                "    total = (CFG['w_correct'] * correct + \n",
                "             CFG['w_trace'] * trace + \n",
                "             CFG['w_conf'] * conf)\n",
                "    \n",
                "    return total, {'correct': correct, 'trace': trace, 'conf': conf}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test reward function\n",
                "test_output = \"\"\"<reasoning>\n",
                "Step 1: We have 45 apples.\n",
                "Step 2: We sell 12 apples.\n",
                "Step 3: Therefore, 45 - 12 = 33 apples remain.\n",
                "</reasoning>\n",
                "<answer>33</answer>\"\"\"\n",
                "\n",
                "reward, components = composite_reward(test_output, \"33\")\n",
                "print(f\"Test reward: {reward:.3f}\")\n",
                "print(f\"Components: {components}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. GRPO Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training state\n",
                "start_time = time.time()\n",
                "last_save_time = start_time\n",
                "global_step = 0\n",
                "best_reward = -float('inf')\n",
                "\n",
                "# Metrics tracking\n",
                "reward_history = []\n",
                "accuracy_history = []\n",
                "\n",
                "os.makedirs('checkpoints/rl', exist_ok=True)\n",
                "os.makedirs('logs/eval_reports', exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sample_batch(data, batch_size):\n",
                "    \"\"\"Sample a batch of prompts.\"\"\"\n",
                "    indices = np.random.choice(len(data), batch_size, replace=False)\n",
                "    return [data[i] for i in indices]\n",
                "\n",
                "def generate_responses(prompts, num_gen=4):\n",
                "    \"\"\"Generate multiple responses per prompt.\"\"\"\n",
                "    # Placeholder - replace with actual generation\n",
                "    responses = []\n",
                "    for prompt in prompts:\n",
                "        prompt_responses = []\n",
                "        for _ in range(num_gen):\n",
                "            # response = model.generate(prompt, temperature=CFG['temperature'])\n",
                "            response = \"<reasoning>Step 1: Calculate.</reasoning><answer>42</answer>\"\n",
                "            prompt_responses.append(response)\n",
                "        responses.append(prompt_responses)\n",
                "    return responses\n",
                "\n",
                "def compute_advantages(rewards):\n",
                "    \"\"\"GRPO: advantage = reward - mean(group).\"\"\"\n",
                "    mean_reward = sum(rewards) / len(rewards)\n",
                "    return [r - mean_reward for r in rewards]\n",
                "\n",
                "def grpo_update(prompts, responses, advantages):\n",
                "    \"\"\"Apply GRPO policy update.\"\"\"\n",
                "    # Placeholder - replace with actual update\n",
                "    loss = 0.1\n",
                "    return loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_step():\n",
                "    \"\"\"Single GRPO training step.\"\"\"\n",
                "    global global_step, last_save_time, best_reward\n",
                "    \n",
                "    # 1. Sample prompts\n",
                "    batch = sample_batch(train_data, CFG['batch_size'])\n",
                "    prompts = [ex['text'].split('A:\\n')[0] + 'A:\\n' for ex in batch]\n",
                "    refs = [ex.get('reference_answer', '') for ex in batch]\n",
                "    \n",
                "    # 2. Generate G responses per prompt\n",
                "    all_responses = generate_responses(prompts, CFG['num_generations'])\n",
                "    \n",
                "    # 3. Compute rewards\n",
                "    all_rewards = []\n",
                "    all_components = []\n",
                "    for i, responses in enumerate(all_responses):\n",
                "        group_rewards = []\n",
                "        for resp in responses:\n",
                "            r, comp = composite_reward(resp, refs[i])\n",
                "            group_rewards.append(r)\n",
                "            all_components.append(comp)\n",
                "        all_rewards.append(group_rewards)\n",
                "    \n",
                "    # 4. Compute advantages\n",
                "    all_advantages = []\n",
                "    for group_rewards in all_rewards:\n",
                "        all_advantages.extend(compute_advantages(group_rewards))\n",
                "    \n",
                "    # 5. GRPO update\n",
                "    flat_responses = [r for group in all_responses for r in group]\n",
                "    flat_prompts = [p for p in prompts for _ in range(CFG['num_generations'])]\n",
                "    loss = grpo_update(flat_prompts, flat_responses, all_advantages)\n",
                "    \n",
                "    # Metrics\n",
                "    avg_reward = np.mean([r for group in all_rewards for r in group])\n",
                "    avg_correct = np.mean([c['correct'] for c in all_components])\n",
                "    \n",
                "    global_step += 1\n",
                "    reward_history.append(avg_reward)\n",
                "    accuracy_history.append(avg_correct)\n",
                "    \n",
                "    return loss, avg_reward, avg_correct"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_checkpoint(name, metrics=None):\n",
                "    path = f\"checkpoints/rl/{name}\"\n",
                "    os.makedirs(path, exist_ok=True)\n",
                "    \n",
                "    meta = {\n",
                "        'step': global_step,\n",
                "        'time': datetime.now().isoformat(),\n",
                "        'config': CFG,\n",
                "        'metrics': metrics or {}\n",
                "    }\n",
                "    with open(f\"{path}/metadata.json\", 'w') as f:\n",
                "        json.dump(meta, f, indent=2)\n",
                "    \n",
                "    # model.save_pretrained(path)\n",
                "    print(f\" Saved: {path}\")\n",
                "\n",
                "def evaluate(num_samples=50):\n",
                "    \"\"\"Run evaluation on val set.\"\"\"\n",
                "    correct = 0\n",
                "    format_ok = 0\n",
                "    trace_scores = []\n",
                "    \n",
                "    samples = val_data[:num_samples]\n",
                "    for ex in samples:\n",
                "        prompt = ex['text'].split('A:\\n')[0] + 'A:\\n'\n",
                "        ref = ex.get('reference_answer', '')\n",
                "        \n",
                "        # output = model.generate(prompt)\n",
                "        output = \"<reasoning>Step.</reasoning><answer>42</answer>\"\n",
                "        \n",
                "        if correctness_score(output, ref) > 0.5:\n",
                "            correct += 1\n",
                "        if '<reasoning>' in output and '<answer>' in output:\n",
                "            format_ok += 1\n",
                "        trace_scores.append(trace_structure_score(output))\n",
                "    \n",
                "    return {\n",
                "        'accuracy': correct / len(samples),\n",
                "        'format_rate': format_ok / len(samples),\n",
                "        'avg_trace': np.mean(trace_scores)\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Main training loop\n",
                "print(\" Starting GRPO Training...\")\n",
                "print(f\"   Updates: {CFG['num_updates']}\")\n",
                "print(f\"   G (generations/prompt): {CFG['num_generations']}\")\n",
                "print()\n",
                "\n",
                "for step in range(CFG['num_updates']):\n",
                "    loss, reward, accuracy = train_step()\n",
                "    \n",
                "    # Log\n",
                "    if global_step % CFG['log_steps'] == 0:\n",
                "        elapsed = (time.time() - start_time) / 60\n",
                "        print(f\"Step {global_step} | Reward: {reward:.3f} | Acc: {accuracy:.2%} | Time: {elapsed:.1f}m\")\n",
                "    \n",
                "    # Evaluate\n",
                "    if global_step % CFG['eval_steps'] == 0:\n",
                "        metrics = evaluate()\n",
                "        print(f\"   Eval - Acc: {metrics['accuracy']:.2%}, Format: {metrics['format_rate']:.2%}\")\n",
                "        \n",
                "        # Save best\n",
                "        composite = 0.6 * metrics['accuracy'] + 0.25 * metrics['avg_trace'] + 0.15 * metrics['format_rate']\n",
                "        if composite > best_reward:\n",
                "            best_reward = composite\n",
                "            save_checkpoint('best', metrics)\n",
                "    \n",
                "    # Periodic save\n",
                "    if (time.time() - last_save_time) > CFG['save_minutes'] * 60:\n",
                "        save_checkpoint(f\"step_{global_step}\")\n",
                "        last_save_time = time.time()\n",
                "\n",
                "print(\"\\n GRPO Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total_time = (time.time() - start_time) / 60\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"GRPO TRAINING COMPLETE\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Total time: {total_time:.1f} minutes\")\n",
                "print(f\"Steps: {global_step}\")\n",
                "print(f\"Best composite score: {best_reward:.3f}\")\n",
                "print(f\"Final avg reward: {np.mean(reward_history[-100:]):.3f}\")\n",
                "print(f\"Final accuracy: {np.mean(accuracy_history[-100:]):.2%}\")\n",
                "print(\"\\n Proceed to: 04_evaluation_and_export.ipynb\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

