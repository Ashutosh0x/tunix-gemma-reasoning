{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#  Data Preparation & Tokenization\n",
                "\n",
                "Load datasets, format with `<reasoning>` tags, and pre-tokenize for training.\n",
                "\n",
                "**Time estimate:** ~30-40 minutes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer\n",
                "\n",
                "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load GSM8K Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load GSM8K from Hugging Face\n",
                "gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
                "print(f\"GSM8K train: {len(gsm8k['train'])} examples\")\n",
                "print(f\"GSM8K test: {len(gsm8k['test'])} examples\")\n",
                "\n",
                "# Show sample\n",
                "print(\"\\n Sample:\")\n",
                "sample = gsm8k['train'][0]\n",
                "print(f\"Question: {sample['question'][:200]}...\")\n",
                "print(f\"Answer: {sample['answer'][:200]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Format Data with Reasoning Tags"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_gsm8k_example(example):\n",
                "    \"\"\"Convert GSM8K to <reasoning>/<answer> format.\"\"\"\n",
                "    question = example['question']\n",
                "    full_answer = example['answer']\n",
                "    \n",
                "    # GSM8K uses #### to separate reasoning from final answer\n",
                "    if '####' in full_answer:\n",
                "        reasoning, final_answer = full_answer.rsplit('####', 1)\n",
                "        reasoning = reasoning.strip()\n",
                "        final_answer = final_answer.strip()\n",
                "    else:\n",
                "        reasoning = full_answer\n",
                "        final_answer = full_answer.split('\\n')[-1].strip()\n",
                "    \n",
                "    formatted = f\"\"\"Q: {question}\n",
                "A:\n",
                "<reasoning>{reasoning}</reasoning>\n",
                "<answer>{final_answer}</answer>\"\"\"\n",
                "    \n",
                "    return {\n",
                "        'text': formatted,\n",
                "        'reference_answer': final_answer,\n",
                "        'domain': 'math'\n",
                "    }\n",
                "\n",
                "# Test formatting\n",
                "formatted_sample = format_gsm8k_example(gsm8k['train'][0])\n",
                "print(\" Formatted example:\")\n",
                "print(formatted_sample['text'][:500])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Format entire dataset\n",
                "train_formatted = [format_gsm8k_example(ex) for ex in gsm8k['train']]\n",
                "test_formatted = [format_gsm8k_example(ex) for ex in gsm8k['test']]\n",
                "\n",
                "print(f\" Formatted {len(train_formatted)} train, {len(test_formatted)} test\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Create Train/Val Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "random.seed(42)\n",
                "\n",
                "# Shuffle and split\n",
                "shuffled = train_formatted.copy()\n",
                "random.shuffle(shuffled)\n",
                "\n",
                "val_size = 200  # Small validation set\n",
                "train_data = shuffled[val_size:]\n",
                "val_data = shuffled[:val_size]\n",
                "\n",
                "print(f\"Train: {len(train_data)} examples\")\n",
                "print(f\"Validation: {len(val_data)} examples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Save Prepared Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs('data/prepared', exist_ok=True)\n",
                "\n",
                "# Save as JSONL\n",
                "def save_jsonl(data, path):\n",
                "    with open(path, 'w', encoding='utf-8') as f:\n",
                "        for item in data:\n",
                "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "\n",
                "save_jsonl(train_data, 'data/prepared/train.jsonl')\n",
                "save_jsonl(val_data, 'data/prepared/valid.jsonl')\n",
                "save_jsonl(test_formatted, 'data/prepared/test.jsonl')\n",
                "\n",
                "print(\" Saved:\")\n",
                "print(f\"  - data/prepared/train.jsonl ({len(train_data)} examples)\")\n",
                "print(f\"  - data/prepared/valid.jsonl ({len(val_data)} examples)\")\n",
                "print(f\"  - data/prepared/test.jsonl ({len(test_formatted)} examples)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Tokenize Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MAX_LENGTH = 1024  # Max tokens\n",
                "\n",
                "def tokenize_example(example):\n",
                "    \"\"\"Tokenize a single example.\"\"\"\n",
                "    tokens = tokenizer(\n",
                "        example['text'],\n",
                "        truncation=True,\n",
                "        max_length=MAX_LENGTH,\n",
                "        return_tensors=None\n",
                "    )\n",
                "    return {\n",
                "        'input_ids': tokens['input_ids'],\n",
                "        'attention_mask': tokens['attention_mask'],\n",
                "        'reference_answer': example.get('reference_answer', ''),\n",
                "        'domain': example.get('domain', 'general')\n",
                "    }\n",
                "\n",
                "# Tokenize all\n",
                "train_tokenized = [tokenize_example(ex) for ex in train_data]\n",
                "val_tokenized = [tokenize_example(ex) for ex in val_data]\n",
                "\n",
                "print(f\" Tokenized {len(train_tokenized)} train, {len(val_tokenized)} val\")\n",
                "\n",
                "# Stats\n",
                "lengths = [len(ex['input_ids']) for ex in train_tokenized]\n",
                "print(f\"\\nToken length stats:\")\n",
                "print(f\"  Min: {min(lengths)}, Max: {max(lengths)}, Avg: {sum(lengths)/len(lengths):.1f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save tokenized data\n",
                "os.makedirs('data/tokenized', exist_ok=True)\n",
                "\n",
                "save_jsonl(train_tokenized, 'data/tokenized/train.jsonl')\n",
                "save_jsonl(val_tokenized, 'data/tokenized/valid.jsonl')\n",
                "\n",
                "print(\" Saved tokenized data to data/tokenized/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Verify Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify we can decode back\n",
                "sample = train_tokenized[0]\n",
                "decoded = tokenizer.decode(sample['input_ids'])\n",
                "\n",
                "print(\" Sample decoded:\")\n",
                "print(decoded[:800])\n",
                "\n",
                "# Check format compliance\n",
                "has_reasoning = '<reasoning>' in decoded and '</reasoning>' in decoded\n",
                "has_answer = '<answer>' in decoded and '</answer>' in decoded\n",
                "print(f\"\\n Has <reasoning> tags: {has_reasoning}\")\n",
                "print(f\" Has <answer> tags: {has_answer}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"DATA PREPARATION COMPLETE\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Train examples: {len(train_tokenized)}\")\n",
                "print(f\"Val examples: {len(val_tokenized)}\")\n",
                "print(\"\\n Proceed to: 02_sft_training.ipynb\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

